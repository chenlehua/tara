# ============================================
# TARA System - GPU Services (AI Models)
# Docker Compose 配置文件
# ============================================
# 需要 NVIDIA GPU 和 nvidia-docker2
# 安装: https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html
# ============================================

version: '3.8'

services:
  # ==================== vLLM 推理服务 ====================
  vllm-inference:
    image: vllm/vllm-openai:latest
    container_name: tara-vllm
    restart: unless-stopped
    ports:
      - "8100:8000"
    volumes:
      - ${MODEL_PATH:-/data/models}:/models
      - vllm-cache:/root/.cache
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - HF_HOME=/models/huggingface
    command: >
      --model ${VLLM_MODEL:-Qwen/Qwen2.5-7B-Instruct}
      --served-model-name qwen
      --host 0.0.0.0
      --port 8000
      --tensor-parallel-size ${TENSOR_PARALLEL_SIZE:-1}
      --gpu-memory-utilization ${GPU_MEMORY_UTILIZATION:-0.9}
      --max-model-len ${MAX_MODEL_LEN:-8192}
      --trust-remote-code
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    networks:
      - tara-network

  # ==================== Embedding 服务 ====================
  embedding-service:
    image: vllm/vllm-openai:latest
    container_name: tara-embedding
    restart: unless-stopped
    ports:
      - "8101:8000"
    volumes:
      - ${MODEL_PATH:-/data/models}:/models
      - embedding-cache:/root/.cache
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - HF_HOME=/models/huggingface
    command: >
      --model ${EMBEDDING_MODEL:-BAAI/bge-large-zh-v1.5}
      --served-model-name embedding
      --host 0.0.0.0
      --port 8000
      --trust-remote-code
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    networks:
      - tara-network

  # ==================== OCR 服务 ====================
  ocr-service:
    image: paddlepaddle/paddle:2.5.2-gpu-cuda11.7-cudnn8.4-trt8.4
    container_name: tara-ocr
    restart: unless-stopped
    ports:
      - "8102:8866"
    volumes:
      - ${MODEL_PATH:-/data/models}/paddleocr:/root/.paddleocr
      - ocr-cache:/root/.cache
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    command: >
      bash -c "pip install paddleocr paddlehub -i https://mirrors.aliyun.com/pypi/simple/ &&
      hub serving start -m ocr_system --use_gpu"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      - tara-network

  # ==================== Triton Inference Server (可选) ====================
  # triton-server:
  #   image: nvcr.io/nvidia/tritonserver:23.12-py3
  #   container_name: tara-triton
  #   restart: unless-stopped
  #   ports:
  #     - "8103:8000"  # HTTP
  #     - "8104:8001"  # gRPC
  #     - "8105:8002"  # Metrics
  #   volumes:
  #     - ${MODEL_PATH:-/data/models}/triton:/models
  #   command: tritonserver --model-repository=/models --strict-model-config=false
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: all
  #             capabilities: [gpu]
  #   networks:
  #     - tara-network

volumes:
  vllm-cache:
    driver: local
  embedding-cache:
    driver: local
  ocr-cache:
    driver: local

networks:
  tara-network:
    external: true
    name: tara-network
