"""
Chat Service
============

Service for AI chat functionality.
"""

import json
import uuid
from datetime import datetime
from typing import Any, AsyncGenerator, Dict, List, Optional

import httpx
from tara_shared.config import settings
from tara_shared.database.redis import get_cache_service
from tara_shared.utils import get_logger

logger = get_logger(__name__)

# In-memory fallback for chat history when Redis is unavailable
_chat_history_store: Dict[str, List[Dict[str, Any]]] = {}


class ChatService:
    """Service for AI chat operations."""

    SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ±½è½¦ç½‘ç»œå®‰å…¨åˆ†æžåŠ©æ‰‹ï¼Œä¸“æ³¨äºŽå¨èƒåˆ†æžä¸Žé£Žé™©è¯„ä¼°(TARA)ã€‚
ä½ çš„èŒè´£åŒ…æ‹¬ï¼š
1. å¸®åŠ©ç”¨æˆ·ç†è§£æ±½è½¦ç½‘ç»œå®‰å…¨æ¦‚å¿µ
2. ååŠ©è¿›è¡Œå¨èƒè¯†åˆ«å’ŒSTRIDEåˆ†æž
3. æŒ‡å¯¼æ”»å‡»è·¯å¾„åˆ†æžå’Œé£Žé™©è¯„ä¼°
4. æä¾›ç¬¦åˆISO/SAE 21434æ ‡å‡†çš„å»ºè®®

è¯·ç”¨ä¸“ä¸šä½†æ˜“æ‡‚çš„æ–¹å¼å›žç­”é—®é¢˜ã€‚å¦‚æžœç”¨æˆ·é—®çš„é—®é¢˜ä¸Žæ±½è½¦ç½‘ç»œå®‰å…¨æ— å…³ï¼Œä½ ä¹Ÿå¯ä»¥å‹å¥½åœ°å›žç­”ï¼Œä½†ä¼šæç¤ºè¿™ä¸æ˜¯ä½ çš„ä¸“ä¸šé¢†åŸŸã€‚"""

    def __init__(self):
        self.cache = get_cache_service()

    async def chat(
        self,
        messages: List[Dict[str, str]],
        project_id: Optional[int] = None,
        context: Optional[Dict[str, Any]] = None,
    ) -> str:
        """Send chat messages and get response."""
        # Prepare messages with system prompt
        full_messages = [{"role": "system", "content": self.SYSTEM_PROMPT}]

        # Add project context if provided
        if project_id:
            project_context = await self._get_project_context(project_id)
            if project_context:
                full_messages.append(
                    {
                        "role": "system",
                        "content": f"å½“å‰é¡¹ç›®ä¸Šä¸‹æ–‡ï¼š\n{project_context}",
                    }
                )

        # Add extra context if provided
        if context:
            context_str = self._format_context(context)
            if context_str:
                full_messages.append(
                    {
                        "role": "system",
                        "content": f"é¢å¤–ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š\n{context_str}",
                    }
                )

        # Add user messages
        full_messages.extend(messages)

        # Save user message to history
        if messages:
            await self._save_message(
                project_id=project_id,
                role="user",
                content=messages[-1].get("content", ""),
            )

        try:
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    f"{settings.qwen3_url}/chat/completions",
                    json={
                        "model": "qwen3",
                        "messages": full_messages,
                        "temperature": 0.7,
                        "max_tokens": 2000,
                    },
                    timeout=60.0,
                )
                response.raise_for_status()
                result = response.json()
                assistant_response = result["choices"][0]["message"]["content"]

                # Save assistant response to history
                await self._save_message(
                    project_id=project_id,
                    role="assistant",
                    content=assistant_response,
                )

                return assistant_response

        except httpx.ConnectError:
            logger.warning("AI service not available, using fallback response")
            return self._get_fallback_response(messages)
        except Exception as e:
            logger.error(f"Chat request failed: {e}")
            return self._get_fallback_response(messages)

    async def chat_stream(
        self,
        messages: List[Dict[str, str]],
        project_id: Optional[int] = None,
    ) -> AsyncGenerator[str, None]:
        """Stream chat response."""
        full_messages = [{"role": "system", "content": self.SYSTEM_PROMPT}]

        if project_id:
            context = await self._get_project_context(project_id)
            if context:
                full_messages.append(
                    {
                        "role": "system",
                        "content": f"å½“å‰é¡¹ç›®ä¸Šä¸‹æ–‡ï¼š\n{context}",
                    }
                )

        full_messages.extend(messages)

        try:
            async with httpx.AsyncClient() as client:
                async with client.stream(
                    "POST",
                    f"{settings.qwen3_url}/chat/completions",
                    json={
                        "model": "qwen3",
                        "messages": full_messages,
                        "temperature": 0.7,
                        "max_tokens": 2000,
                        "stream": True,
                    },
                    timeout=120.0,
                ) as response:
                    async for line in response.aiter_lines():
                        if line.startswith("data: "):
                            data = line[6:]
                            if data == "[DONE]":
                                break
                            try:
                                chunk = json.loads(data)
                                content = (
                                    chunk.get("choices", [{}])[0]
                                    .get("delta", {})
                                    .get("content", "")
                                )
                                if content:
                                    yield content
                            except Exception:
                                continue

        except httpx.ConnectError:
            logger.warning("AI service not available for streaming")
            yield self._get_fallback_response(messages)
        except Exception as e:
            logger.error(f"Stream chat failed: {e}")
            yield "æŠ±æ­‰ï¼ŒAIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åŽå†è¯•ã€‚"

    async def get_history(
        self, project_id: Optional[int] = None
    ) -> List[Dict[str, Any]]:
        """Get chat history for a project."""
        history_key = self._get_history_key(project_id)

        # Try to get from Redis cache
        if self.cache and self.cache.is_available():
            try:
                cached = self.cache.get(history_key)
                if cached:
                    return json.loads(cached)
            except Exception as e:
                logger.warning(f"Failed to get history from cache: {e}")

        # Fallback to in-memory store
        return _chat_history_store.get(history_key, [])

    async def clear_history(self, project_id: Optional[int] = None) -> None:
        """Clear chat history for a project."""
        history_key = self._get_history_key(project_id)

        # Clear from Redis cache
        if self.cache and self.cache.is_available():
            try:
                self.cache.delete(history_key)
            except Exception as e:
                logger.warning(f"Failed to clear history from cache: {e}")

        # Clear from in-memory store
        if history_key in _chat_history_store:
            del _chat_history_store[history_key]

    async def _save_message(
        self,
        project_id: Optional[int],
        role: str,
        content: str,
    ) -> None:
        """Save a message to history."""
        history_key = self._get_history_key(project_id)

        message = {
            "id": str(uuid.uuid4()),
            "role": role,
            "content": content,
            "timestamp": datetime.now().isoformat(),
        }

        # Get existing history
        history = await self.get_history(project_id)
        history.append(message)

        # Keep only last 50 messages
        if len(history) > 50:
            history = history[-50:]

        # Try to save to Redis cache
        if self.cache and self.cache.is_available():
            try:
                self.cache.set(
                    history_key, json.dumps(history), expire=86400
                )  # 24 hours
            except Exception as e:
                logger.warning(f"Failed to save history to cache: {e}")

        # Also save to in-memory store
        _chat_history_store[history_key] = history

    def _get_history_key(self, project_id: Optional[int]) -> str:
        """Get the cache key for chat history."""
        if project_id:
            return f"chat_history:project:{project_id}"
        return "chat_history:global"

    def _format_context(self, context: Dict[str, Any]) -> str:
        """Format context dictionary to string."""
        if not context:
            return ""

        parts = []
        if "action" in context:
            parts.append(f"æ“ä½œç±»åž‹: {context['action']}")
        if "asset_id" in context:
            parts.append(f"èµ„äº§ID: {context['asset_id']}")
        if "threat_id" in context:
            parts.append(f"å¨èƒID: {context['threat_id']}")

        return "\n".join(parts) if parts else ""

    def _get_fallback_response(self, messages: List[Dict[str, str]]) -> str:
        """Get a fallback response when AI service is unavailable."""
        if not messages:
            return "æ‚¨å¥½ï¼æˆ‘æ˜¯TARA AIåŠ©æ‰‹ï¼Œè¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨çš„ï¼Ÿ"

        user_message = messages[-1].get("content", "").lower()

        # Simple keyword-based responses
        if "stride" in user_message or "å¨èƒ" in user_message:
            return """STRIDEæ˜¯ä¸€ç§å¨èƒå»ºæ¨¡æ–¹æ³•ï¼ŒåŒ…æ‹¬å…­ç±»å¨èƒï¼š

1. **S - Spoofing (æ¬ºéª—)**: èº«ä»½ä¼ªé€ ï¼Œå†’å……åˆæ³•ç”¨æˆ·æˆ–ç³»ç»Ÿ
2. **T - Tampering (ç¯¡æ”¹)**: æ¶æ„ä¿®æ”¹æ•°æ®æˆ–ä»£ç 
3. **R - Repudiation (å¦è®¤)**: å¦è®¤å·²æ‰§è¡Œçš„æ“ä½œ
4. **I - Information Disclosure (ä¿¡æ¯æ³„éœ²)**: æœªæŽˆæƒè®¿é—®æ•æ„Ÿä¿¡æ¯
5. **D - Denial of Service (æ‹’ç»æœåŠ¡)**: ä½¿ç³»ç»Ÿæˆ–æœåŠ¡ä¸å¯ç”¨
6. **E - Elevation of Privilege (æƒé™æå‡)**: èŽ·å–æœªæŽˆæƒçš„æƒé™

å¯¹äºŽæ±½è½¦ç½‘ç»œå®‰å…¨ï¼Œæ¯ç§å¨èƒéƒ½éœ€è¦æ ¹æ®å…·ä½“èµ„äº§å’Œæ”»å‡»é¢è¿›è¡Œåˆ†æžã€‚"""

        elif "cal" in user_message or "é£Žé™©" in user_message or "ç­‰çº§" in user_message:
            return """CAL (Cybersecurity Assurance Level) æ˜¯ISO 21434æ ‡å‡†ä¸­çš„ç½‘ç»œå®‰å…¨ä¿éšœç­‰çº§ï¼š

- **CAL 1**: åŸºç¡€å®‰å…¨ä¿éšœ - é€‚ç”¨äºŽä½Žé£Žé™©åœºæ™¯
- **CAL 2**: æ ‡å‡†å®‰å…¨ä¿éšœ - ç³»ç»ŸåŒ–çš„å®‰å…¨å¼€å‘æµç¨‹
- **CAL 3**: é«˜çº§å®‰å…¨ä¿éšœ - ä¸¥æ ¼çš„å®‰å…¨æµ‹è¯•å’Œå®¡è®¡
- **CAL 4**: æœ€é«˜å®‰å…¨ä¿éšœ - å½¢å¼åŒ–éªŒè¯å’Œç‹¬ç«‹å®¡è®¡

CALç­‰çº§ç”±é£Žé™©è¯„ä¼°ç»“æžœå†³å®šï¼Œè€ƒè™‘å½±å“(Impact)å’Œå¯è¡Œæ€§(Feasibility)ã€‚"""

        elif "iso" in user_message or "21434" in user_message or "æ ‡å‡†" in user_message:
            return """ISO/SAE 21434æ˜¯æ±½è½¦ç½‘ç»œå®‰å…¨å·¥ç¨‹æ ‡å‡†ï¼Œä¸»è¦å†…å®¹åŒ…æ‹¬ï¼š

1. **ç»„ç»‡ç½‘ç»œå®‰å…¨ç®¡ç†**: å»ºç«‹ç½‘ç»œå®‰å…¨æ–‡åŒ–å’Œæ²»ç†
2. **é¡¹ç›®ä¾èµ–çš„ç½‘ç»œå®‰å…¨ç®¡ç†**: ç®¡ç†ä¾›åº”é“¾å®‰å…¨
3. **åˆ†å¸ƒå¼ç½‘ç»œå®‰å…¨æ´»åŠ¨**: è·¨ç»„ç»‡åä½œ
4. **æŒç»­çš„ç½‘ç»œå®‰å…¨æ´»åŠ¨**: ç›‘æŽ§ã€å“åº”å’Œæ›´æ–°
5. **æ¦‚å¿µé˜¶æ®µ**: å®šä¹‰ç½‘ç»œå®‰å…¨ç›®æ ‡
6. **äº§å“å¼€å‘é˜¶æ®µ**: TARAåˆ†æžå’Œå®‰å…¨è®¾è®¡
7. **ç”Ÿäº§å’Œè¿ç»´**: ç¡®ä¿ç”Ÿäº§çŽ¯å¢ƒå®‰å…¨

è¯¥æ ‡å‡†å¼ºè°ƒå…¨ç”Ÿå‘½å‘¨æœŸçš„ç½‘ç»œå®‰å…¨ç®¡ç†ã€‚"""

        elif "æ”»å‡»" in user_message or "è·¯å¾„" in user_message:
            return """æ”»å‡»è·¯å¾„åˆ†æžæ˜¯è¯„ä¼°æ”»å‡»å¯è¡Œæ€§çš„é‡è¦æ–¹æ³•ï¼Œéœ€è¦è€ƒè™‘ï¼š

1. **æ”»å‡»æ½œåŠ›å‚æ•°**:
   - ä¸“ä¸šçŸ¥è¯† (Expertise)
   - æ‰€éœ€æ—¶é—´ (Elapsed Time)
   - æ‰€éœ€è®¾å¤‡ (Equipment)
   - ç›®æ ‡çŸ¥è¯† (Knowledge of Target)
   - æ”»å‡»çª—å£ (Window of Opportunity)

2. **å¯è¡Œæ€§è¯„çº§**:
   - æ”»å‡»æ½œåŠ› 0-9: é«˜å¯è¡Œæ€§
   - æ”»å‡»æ½œåŠ› 10-17: ä¸­å¯è¡Œæ€§
   - æ”»å‡»æ½œåŠ› 18-24: ä½Žå¯è¡Œæ€§
   - æ”»å‡»æ½œåŠ› 25+: æžä½Žå¯è¡Œæ€§

å»ºè®®ç»“åˆå…·ä½“èµ„äº§å’Œå¨èƒåœºæ™¯è¿›è¡Œè¯¦ç»†åˆ†æžã€‚"""

        else:
            return """æ‚¨å¥½ï¼æˆ‘æ˜¯TARA AIåŠ©æ‰‹ï¼Œä¸“æ³¨äºŽæ±½è½¦ç½‘ç»œå®‰å…¨åˆ†æžã€‚

æˆ‘å¯ä»¥å¸®åŠ©æ‚¨ï¼š
- ðŸ” **å¨èƒè¯†åˆ«**: ä½¿ç”¨STRIDEæ–¹æ³•åˆ†æžæ½œåœ¨å¨èƒ
- ðŸ“Š **é£Žé™©è¯„ä¼°**: è®¡ç®—CALç­‰çº§å’Œé£Žé™©å€¼
- ðŸ›¡ï¸ **å®‰å…¨æŽªæ–½**: æŽ¨èç¬¦åˆISO 21434çš„æŽ§åˆ¶æŽªæ–½
- ðŸ”— **æ”»å‡»è·¯å¾„**: åˆ†æžæ”»å‡»å¯è¡Œæ€§å’Œæ½œåŠ›

è¯·é—®æ‚¨éœ€è¦ä»€ä¹ˆå¸®åŠ©ï¼Ÿ"""

    async def _get_project_context(self, project_id: int) -> Optional[str]:
        """Get project context for chat."""
        try:
            async with httpx.AsyncClient() as client:
                response = await client.get(
                    f"http://project-service:8001/api/v1/projects/{project_id}",
                    timeout=10.0,
                )
                if response.status_code == 200:
                    data = response.json().get("data", {})
                    return f"""é¡¹ç›®: {data.get('name', 'Unknown')}
è½¦åž‹: {data.get('vehicle_type', 'Unknown')} {data.get('vehicle_model', '')}
æ ‡å‡†: {data.get('standard', 'ISO/SAE 21434')}
èŒƒå›´: {data.get('scope', 'N/A')}"""
        except Exception as e:
            logger.debug(f"Failed to get project context: {e}")

        return None
